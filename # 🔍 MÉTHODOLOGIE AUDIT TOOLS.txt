









# ğŸ” MÃ‰THODOLOGIE AUDIT TOOLS

## Ã‰tat: 8/40 auditÃ©s
- discord_bot: 9.6/10 âœ…
- ship_tracker: 9.4/10 âœ…
- youtube_download: 9.3/10 âœ…
- pdf_search: 8.8/10 âœ…
- chess_com: 8.8/10 âœ…
- date: 8.5/10 âœ…
- random: crÃ©Ã© âœ…

---

## PROCESS (9 Ã©tapes)

### 1. SÃ‰LECTION RANDOM
- Utiliser le tool `random`: `pick_random` avec la liste des 32 tools restants
- Simple et rapide

### 2. CHARGEMENT
```
load_file: [
  "src/tools/{tool}.py",
  "src/tool_specs/{tool}.json", 
  "src/tools/_{tool}/*.py"  // TOUS
]
```

### 3. TESTS PRÃ‰LIMINAIRES + AUDIT ğŸ†•

**NOUVEAU** : Tester le tool AVANT d'auditer le code pour comprendre son comportement rÃ©el.

#### A. TESTS PRÃ‰LIMINAIRES (AVANT AUDIT)

**Objectif** : Comprendre comment le tool fonctionne rÃ©ellement, identifier les bugs Ã©vidents.

**Ã‰tapes** :

1. **Reload tools**
   ```bash
   GET /tools?reload=1
   ```

2. **Lire la spec JSON rapidement**
   - Quelles opÃ©rations disponibles ?
   - Quels paramÃ¨tres requis ?
   - Y a-t-il des defaults ?

3. **Tester 3-5 opÃ©rations reprÃ©sentatives**
   - OpÃ©ration la plus simple (ex: get_info, list, health_check)
   - OpÃ©ration avec paramÃ¨tres (ex: search avec query)
   - OpÃ©ration avec limites (si applicable)
   - OpÃ©ration edge case (paramÃ¨tre invalide, limite dÃ©passÃ©e)
   - OpÃ©ration complexe (si applicable)

**Template de test** :
```python
# Test 1: OpÃ©ration simple (baseline)
POST /execute {
  "tool": "{tool}",
  "params": {
    "operation": "xxx"
  }
}

# Test 2: OpÃ©ration avec params valides
POST /execute {
  "tool": "{tool}",
  "params": {
    "operation": "yyy",
    "param1": "value",
    "limit": 5
  }
}

# Test 3: Validation (paramÃ¨tre invalide)
POST /execute {
  "tool": "{tool}",
  "params": {
    "operation": "yyy",
    "param1": "INVALID_VALUE"
  }
}

# Test 4: Edge case (limite dÃ©passÃ©e)
POST /execute {
  "tool": "{tool}",
  "params": {
    "operation": "yyy",
    "limit": 99999
  }
}

# Test 5: OpÃ©ration complexe (si applicable)
POST /execute {
  "tool": "{tool}",
  "params": {
    "operation": "zzz",
    ...
  }
}
```

4. **Noter les observations** :
   - âœ… Outputs propres ou verbeux ?
   - âœ… Erreurs claires ?
   - âœ… Validation stricte ?
   - âœ… Truncation warnings ?
   - âœ… Counts clairs (total vs returned) ?
   - âœ… Timeouts raisonnables ?
   - âš ï¸ Bugs dÃ©tectÃ©s ?
   - âš ï¸ Comportement inattendu ?

**Format d'affichage** :
```
=== TESTS PRÃ‰LIMINAIRES ===

âœ… Test 1 (get_info): OK
   Output: {rÃ©sumÃ© 2-3 lignes}

âœ… Test 2 (search): OK
   Output: 10 rÃ©sultats, counts clairs

âš ï¸ Test 3 (validation): PROBLÃˆME
   ParamÃ¨tre invalide acceptÃ©, pas d'erreur !

âœ… Test 4 (edge case): OK
   Limite respectÃ©e, warning affichÃ©

OBSERVATIONS:
- Output verbeux (success: true partout)
- Pas de truncation warnings
- Validation faible sur param1
- Timeouts raisonnables
```

#### B. AUDIT JSON SPEC (CRITIQUE - LLM)

**Maintenant qu'on connaÃ®t le comportement rÃ©el, auditer la spec JSON** :

- [ ] `category` dÃ©finie
- [ ] `displayName` prÃ©sent
- [ ] `additionalProperties: false`
- [ ] Arrays avec `items`
- [ ] Enums complets
- [ ] `description` claire (1 ligne max)
- [ ] `required` corrects (correspond aux tests ?)
- [ ] Defaults sensÃ©s (correspond au comportement ?)
- [ ] Limits min/max (correspondent aux validations rÃ©elles ?)

**Questions critiques** :
- LLM comprend CHAQUE param ?
- Descriptions ambiguÃ«s ?
- Defaults documentÃ©s vs rÃ©els : cohÃ©rents ?
- Enums complets vs code : cohÃ©rents ?
- Params secrets ?

**VÃ©rifier cohÃ©rence JSON â†” TESTS** :
- Les defaults dans JSON correspondent aux tests ?
- Les enums dans JSON correspondent aux opÃ©rations testÃ©es ?
- Les validations dans JSON correspondent aux erreurs obtenues ?

#### C. AUDIT CODE

**Maintenant auditer le code avec le contexte des tests** :

- [ ] Output size: limits + truncation warnings (vu dans les tests ?)
- [ ] Validation stricte (cohÃ©rente avec les tests ?)
- [ ] Try-catch global
- [ ] Timeouts (cohÃ©rents avec les tests ?)
- [ ] Pas side-effects import
- [ ] Logging minimal
- [ ] DRY
- [ ] Modulaire

**Outputs MINIMAUX** :
```python
âœ… return [1,2,3]
âœ… return "heads"

âŒ return {"success": true, "operation": "...", ...}
```

**VÃ©rifier cohÃ©rence CODE â†” TESTS** :
- Le code valide ce que les tests ont montrÃ© ?
- Les erreurs du code correspondent aux erreurs obtenues ?
- Les limites du code correspondent aux tests ?

#### D. NOTATION

| CritÃ¨re | Score | Justification |
|---------|-------|---------------|
| JSON Spec LLM | /10 | BasÃ© sur tests + lecture JSON |
| Architecture | /10 | BasÃ© sur structure fichiers |
| SÃ©curitÃ© | /10 | BasÃ© sur code + comportement |
| Robustesse | /10 | BasÃ© sur tests edge cases |
| ConformitÃ© | /10 | BasÃ© sur cohÃ©rence JSONâ†”codeâ†”tests |
| Performance | /10 | BasÃ© sur timeouts + tests |
| MaintenabilitÃ© | /10 | BasÃ© sur code |
| Documentation | /10 | BasÃ© sur JSON + docstrings |

**Score total** : X.X/10

**IMPORTANT** : Les tests prÃ©liminaires influencent la notation !
- Si validation faible dÃ©tectÃ©e dans tests â†’ Robustesse -2
- Si outputs verbeux dÃ©tectÃ©s â†’ ConformitÃ© -2
- Si pas de truncation warning â†’ Performance -1
- Si erreurs pas claires â†’ Documentation -1

---

### 4. CORRECTIFS

**Prioriser les correctifs basÃ©s sur les tests** :

```
fix({tool}): critical audit fixes (X.Xâ†’Y.Y/10)

ğŸ”´ CRITICAL:
- Bug dÃ©tectÃ© dans test 3: ...
  â†’ BEFORE: accepte valeur invalide
  â†’ AFTER: validation stricte ajoutÃ©e

ğŸŸ¡ MAJOR:
- Output verbeux dÃ©tectÃ© dans tous les tests
  â†’ BEFORE: {"success": true, "data": ...}
  â†’ AFTER: retourne directement data

ğŸŸ¢ IMPROVEMENTS:
- Truncation warnings manquants (test 4)
  â†’ Ajout de warnings explicites

TECHNICAL DETAILS:
- file.py: +XXX bytes
- ConformitÃ©: XX% â†’ YY%

AUDIT SCORE: X.X/10 â†’ Y.Y/10 â­â­â­â­

Part of: [Unreleased] Tools Audit Campaign
```

---

### 5. TESTS DE VALIDATION (APRÃˆS CORRECTIFS)

**Re-tester uniquement les cas problÃ©matiques** pour valider les correctifs :

#### A. Reload tools
```bash
GET /tools?reload=1
```

#### B. Re-tester les cas qui posaient problÃ¨me
- Test qui avait Ã©chouÃ© / montrÃ© un bug
- Test avec output verbeux (vÃ©rifier simplification)
- Test avec validation faible (vÃ©rifier renforcement)
- Test sans truncation warning (vÃ©rifier ajout)

**Format d'affichage** :
```
=== TESTS DE VALIDATION ===

âœ… Test 3 (validation): CORRIGÃ‰
   AVANT: acceptait valeur invalide
   APRÃˆS: {"error": "Invalid param1: must be..."}

âœ… Test 2 (output): SIMPLIFIÃ‰
   AVANT: {"success": true, "data": [...]}
   APRÃˆS: [...]

âœ… Test 4 (truncation): WARNING AJOUTÃ‰
   AVANT: 125 rÃ©sultats, pas de message
   APRÃˆS: {"total": 125, "returned": 50, "truncated": true, "message": "..."}
```

#### C. En cas d'erreur persistante
- Analyser l'erreur
- Corriger Ã  nouveau
- Re-tester
- Documenter dans CHANGELOG (section "Fixed during testing")

---

### 6. CHANGELOG

Ajouter section dans `[Unreleased]`:
```markdown
### {tool} - [DATE] âœ… AUDITED

**Score**: X.X â†’ Y.Y/10

#### Fixed
- Bug 1 (dÃ©tectÃ© dans test 3)
- Bug 2 (dÃ©tectÃ© dans test 4)

#### Improved
- Output simplifiÃ© (tous les tests)
- Validation renforcÃ©e (test 3)

#### Technical Details
- Files modifiÃ©s
- ConformitÃ©: XX% â†’ YY%

#### Tests PrÃ©liminaires
- âœ… Test 1: ...
- âš ï¸ Test 3: bug dÃ©tectÃ©, corrigÃ©
- âœ… Test 4: ...

#### Tests de Validation
- âœ… Test 3: bug corrigÃ©, validation OK
- âœ… Test 4: truncation warning ajoutÃ©

#### Audit Results
[Tableau scores]

**SCORE FINAL: Y.Y/10** â­â­â­â­â­

#### Known Issues (si applicable)
- Issue 1: Description (confirmÃ© Perplexity si externe)
```

**IMPORTANT** : VÃ©rifier la taille du CHANGELOG :
- Si > 10KB : **ROTATION OBLIGATOIRE**
  - CrÃ©er `changelogs/CHANGELOG_{start}_to_{end}.md`
  - Garder uniquement `[Unreleased]` dans CHANGELOG.md racine
  - Note en tÃªte: "Older entries have been archived under changelogs/"

---

### 7. COMMIT + PUSH
```bash
git commit_all -m "fix({tool}): critical audit fixes (X.Xâ†’Y.Y/10)

ğŸ”´ CRITICAL:
- Fix 1 (dÃ©tectÃ© en test)

ğŸŸ¡ MAJOR:
- Fix 2

ğŸŸ¢ IMPROVEMENTS:
- Fix 3

TECHNICAL DETAILS:
- file.py: +XXX bytes
- ConformitÃ©: XX% â†’ YY%

TESTS:
âœ… Test 1: ...
âš ï¸ Test 3: bug dÃ©tectÃ© â†’ corrigÃ©
âœ… Validation: tous les tests passent

AUDIT SCORE: X.X/10 â†’ Y.Y/10 â­â­â­â­

KNOWN ISSUES (si applicable):
- Issue 1

Part of: [Unreleased] Tools Audit Campaign

CHANGELOG: [OK / Rotated to changelogs/XXX]"

git push main
```

---

### 8. UNLOAD WORKSPACE ğŸ§¹

**CRITIQUE** : LibÃ©rer l'espace workspace pour le prochain audit.

```bash
unload_file: [
  "src/tools/{tool}.py",
  "src/tool_specs/{tool}.json",
  "src/tools/_{tool}/*.py"  // TOUS les fichiers chargÃ©s
]
```

**VÃ©rification** :
- Confirmer l'espace libÃ©rÃ© (workspace summary)
- Garder uniquement les fichiers de base (LLM_DEV_GUIDE.md, README.md, CHANGELOG.md, procÃ©dure)

---

### 9. METTRE Ã€ JOUR CETTE PROCÃ‰DURE ğŸ“

**OBLIGATOIRE** : AprÃ¨s chaque audit complet, mettre Ã  jour cette procÃ©dure :

1. **IncrÃ©menter le compteur**
   - "Ã‰tat: 8/40 auditÃ©s" â†’ "Ã‰tat: 9/40 auditÃ©s"

2. **Ajouter le tool Ã  la liste des auditÃ©s**
   ```
   - discord_bot: 9.6/10 âœ…
   - ship_tracker: 9.4/10 âœ…  
   - youtube_download: 9.3/10 âœ…  â† AJOUTER ICI (ordre dÃ©croissant de score)
   ```

3. **Retirer le tool de la liste des restants**
   - Liste "32 tools restants" â†’ "31 tools restants"
   - Supprimer `{tool}` de la liste alphabÃ©tique

4. **Commit la procÃ©dure**
   ```bash
   git commit -m "docs(audit): update methodology - {tool} audited (X/40)"
   git push main
   ```

**Exemple** :
```
# AVANT
Ã‰tat: 8/40 auditÃ©s
32 tools restants: academic_research_super, astronomy, ...

# APRÃˆS (audit de {tool})
Ã‰tat: 9/40 auditÃ©s
- {tool}: X.X/10 âœ…
31 tools restants: academic_research_super, astronomy, ...
```

---

## RÃˆGLES ABSOLUES

1. **TESTS D'ABORD** : ğŸ†• Tester AVANT d'auditer le code (Ã©tape 3A)
2. **JSON** : Auditer lisibilitÃ© LLM + cohÃ©rence avec tests
3. **OUTPUTS** : Minimaux (pas metadata) - dÃ©tectÃ© dans tests
4. **TRUNCATION** : Warnings si > 50 items - vÃ©rifiÃ© dans tests
5. **TESTS VALIDATION** : âš¡ Re-tester aprÃ¨s correctifs (Ã©tape 5)
6. **CHANGELOG** : Toujours mÃ j + rotation si > 10KB
7. **PAS RELEASE** : Juste push
8. **PERPLEXITY** : SystÃ©matique pour les 404 ou bugs suspects d'API externe
9. **UNLOAD** : ğŸ§¹ **TOUJOURS** unload Ã  la fin pour libÃ©rer workspace
10. **PROCÃ‰DURE** : ğŸ“ **TOUJOURS** mettre Ã  jour cette procÃ©dure aprÃ¨s chaque audit

---

## AVANTAGES DE LA NOUVELLE APPROCHE ğŸ†•

âœ… **Comprendre avant d'auditer** : comportement rÃ©el vs spec
âœ… **DÃ©tection prÃ©coce** : bugs dÃ©tectÃ©s avant mÃªme de lire le code
âœ… **Audit ciblÃ©** : focus sur les incohÃ©rences JSONâ†”codeâ†”tests
âœ… **Validation efficace** : re-test uniquement ce qui posait problÃ¨me
âœ… **Documentation prÃ©cise** : CHANGELOG avec rÃ©fÃ©rences aux tests
âœ… **Score plus juste** : basÃ© sur comportement rÃ©el, pas juste le code

---

## COMMANDE REPRISE

```
Audite le prochain tool (random)
```

**Workspace dÃ©jÃ  chargÃ©** : LLM_DEV_GUIDE.md, README.md, CHANGELOG.md, procÃ©dure

**32 tools restants** : academic_research_super, astronomy, aviation_weather, call_llm, coingecko, device_location, discord_webhook, email_send, excel_to_sqlite, ffmpeg_frames, flight_tracker, generate_edit_image, git, gitbook, google_maps, http_client, imap, math, news_aggregator, office_to_pdf, ollama_local, open_meteo, pdf2text, pdf_download, reddit_intelligence, script_executor, sqlite_db, ssh_admin, telegram_bot, trivia_api, universal_doc_scraper, velib, video_transcribe, youtube_search

PrÃªt. ğŸš€

 
 
 
 
 
 
 
