# ğŸ¤– AI/LLM Curation Worker

Worker d'orchestration pour la curation automatisÃ©e des actualitÃ©s IA/LLM (â‰¤ 72h), avec fusion du rapport prÃ©cÃ©dent et dÃ©duplication, gÃ©nÃ©ration dâ€™un rapport FR lisible grand public mais techniquement fiable.

---

## ğŸ“‹ Description

- Collecte multi-sources (News, Reddit, arXiv, Papers With Code, Sonar + blogs/labs selon config)
- FraÃ®cheur stricte: â‰¤ 72 heures (3 jours)
- Double verrou fraÃ®cheur:
  1) Filtrage cÃ´tÃ© API via les paramÃ¨tres MCP (from_date/to_date, etc.)
  2) Filet de sÃ©curitÃ© moteur via transform `filter_by_date` (dÃ©terministe)
- Scoring via GPT (pertinence/novelty/source/diversitÃ©)
- Validation qualitÃ© via Sonar (score â‰¥ 7/10)
- Fusion/dÃ©doublonnage avec le rapport prÃ©cÃ©dent via GPT (sortie JSON stricte), puis format final FR
- Sauvegarde en base unique du worker (`worker_ai_curation.db`)

---

## ğŸ§­ Politique de fraÃ®cheur (â‰¤ 72h)

- Cutoff dynamique: `from_date = now - 3 jours`, `to_date = now` (UTC)
- PrioritÃ© aux filtres API (MCP params) pour rÃ©duire le volume amont:
  - News (news_aggregator): `from_date`, `to_date`, `query` stricte LLM-core
  - Reddit (reddit_intelligence): `time_filter=week` (approx), puis filtre moteur <72h via `created_utc`
  - arXiv (academic_research_super): filtre moteur <72h via `publication_date` (et bornage annÃ©e si utile)
  - Sonar: contrainte dans le prompt (imposer `published_at` ISO et â‰¤72h)
- SÃ©curitÃ© moteur (transform `filter_by_date`): applique la rÃ¨gle <72h de maniÃ¨re dÃ©terministe par source

---

## ğŸ”„ Workflow (v6.0.3)

1) Collecte (API-level filters dâ€™abord)
- News: requÃªte LLM-core + from/to
- Reddit: multi_subs (time_filter=week)
- arXiv: derniers papiers (max_results 30)
- PWC: page â€œlatestâ€
- Sonar: recherche temps rÃ©el (prompt)

2) Normalisation & FraÃ®cheur
- Normalise Sonar (JSON)
- `filter_by_date` par source (news/reddit/arXiv/sonar)
- DÃ©duplication Ã©ventuelle (optionnelle) avant LLM (par URL)

3) Scoring & Validation
- GPT scoring â†’ top10 (mix sources)
- Sonar validation (score >= 7) + retry loop (max 3)

4) Fusion / DÃ©dup rapport
- Charge le rapport prÃ©cÃ©dent (markdown + top10_json)
- GPT â€œmergeâ€ (prompts/gpt_merge_report_fr.json) â†’ JSON: {final_report_markdown, final_top10}
- Parse + utilise `final_top10` pour la sauvegarde

5) Sauvegarde
- DB: `reports(report_markdown, top10_json, avg_score, retry_count, completed_at, ...)`

---

## ğŸ“ RÃ©pertoire

```
workers/ai_curation/
â”œâ”€â”€ main.process.json                 # Process principal
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ worker_ctx.json               # db_name=worker_ai_curation, modÃ¨les, seuils, etc.
â”‚   â””â”€â”€ scopes.json                   # Scopes cycle*
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ gpt_scoring.json              # Scoring (rappel de FROM/TO ISO, JSON only)
â”‚   â”œâ”€â”€ sonar_fetch.json              # Sonar (published_at ISO obligatoire, â‰¤72h)
â”‚   â”œâ”€â”€ sonar_validation.json         # Validation Sonar (score 1-10 + feedback JSON)
â”‚   â””â”€â”€ gpt_merge_report_fr.json      # Fusion/dÃ©doublonnage de rapports (JSON strict)
â””â”€â”€ README.md
```

---

## ğŸ—„ï¸ Base de donnÃ©es (worker_ai_curation.db)

- `validation_logs(id, timestamp, attempt, score, feedback, top10_json)`
- `reports(id, created_at, date_from, date_to, report_markdown, avg_score, retry_count, top10_json, completed_at)`

---

## ğŸš€ Lancement

```python
from src.tools.orchestrator import run

# Start
run(operation="start", worker_name="ai_curation", worker_file="workers/ai_curation/main.process.json", hot_reload=True)

# Status
run(operation="status", worker_name="ai_curation")

# Stop
run(operation="stop", worker_name="ai_curation", stop={"mode": "soft"})
```

---

## ğŸ›ï¸ MCP params (filtres cÃ´tÃ© API)

- News (news_aggregator.search_news)
  - `query`: (LLM OR "large language model" OR transformer OR "fine-tuning") AND (OpenAI OR Anthropic OR DeepMind OR "Google AI" OR Meta)
  - `from_date`: `${cycle.dates.from}` (ISO date YYYY-MM-DD)
  - `to_date`: `${cycle.dates.now}` (ISO date YYYY-MM-DD)
  - `providers`: ["nyt","guardian"], `limit`: 30, pagination si besoin
- Reddit (reddit_intelligence.multi_search)
  - `subreddits`: ["MachineLearning","LocalLLaMA","OpenAI"], `limit_per_sub`: 10, `time_filter`: "week"
  - Filtrage moteur <72h via `created_utc`
- arXiv (academic_research_super.search_papers)
  - `query` LLM-core, `max_results`: 30; filtrage moteur via `publication_date`
- Sonar (call_llm)
  - Prompt impose `published_at` ISO â‰¤72h; filtre moteur `filter_by_date` en plus

---

## ğŸ§ª QualitÃ© & TraÃ§abilitÃ©

- Compteurs par source: `cycle.metrics.*_kept` / `*_dropped` aprÃ¨s `filter_by_date`
- Fusion GPT: retourne JSON strict; on exige `published_at` dans `final_top10[]`
- DÃ©duplication: par URL et par titres quasi-identiques (rÃ¨gle de prompt), + option filet moteur `dedupe_by_url`

---

## ğŸ› ï¸ Transforms utilisÃ©s

- `filter_by_date` (dÃ©terministe, <72h) â€” SÃ‰CURITÃ‰
- `normalize_llm_output`, `extract_field`, `json_stringify`, `sanitize_text`, `increment`, `set_value`

NB: Les transforms sont pures (aucune I/O). Toute I/O passe par les tools MCP (`http_tool`, `sqlite_db`, etc.).

---

## ğŸ“ Changelog

- v6.0.3 (2025-10-18)
  - FraÃ®cheur: filtres API (MCP) + `filter_by_date` moteur
  - Fusion/dÃ©doublonnage GPT entre ancien et nouveau rapport
  - Refactor transforms: 1 fichier = 1 transform (pur)
