# Chat Agent (Threaded Conversations)

**Agent conversationnel persistant avec exÃ©cution d'outils et stockage serveur via Platform Threading API.**

---

## ğŸ¯ Objectif

`chat_agent` permet de mener des conversations longues et persistantes avec un LLM, tout en lui donnant accÃ¨s Ã  des outils (MCP). Contrairement Ã  `call_agent` (client-side state), `chat_agent` stocke **tout l'historique cÃ´tÃ© serveur** via l'API Platform Threads.

### Cas d'usage typiques

âœ… **Chatbots persistants** : L'utilisateur peut quitter et revenir, la conversation reprend oÃ¹ elle s'Ã©tait arrÃªtÃ©e  
âœ… **Assistants longue durÃ©e** : Accumulation de contexte sur plusieurs heures/jours  
âœ… **Multi-device** : AccÃ¨s au mÃªme thread depuis diffÃ©rents clients  
âœ… **Workflows interactifs** : L'agent utilise des outils pour accomplir des tÃ¢ches complexes en plusieurs Ã©tapes

---

## ğŸ“Š DiffÃ©rences avec l'existant

| Feature | `call_llm` | `call_agent` (v1) | `chat_agent` (v2) |
|---------|-----------|-------------------|-------------------|
| **Tours** | 1 (tool use unique) | N (multi-turn) | N (multi-turn) |
| **Persistance** | âŒ Aucune | Client-side (`state`) | âœ… Server-side (`thread_id`) |
| **API** | Stateless | Stateless | Stateful (Threads API) |
| **Contexte** | Manuel | Manuel (`resume_from`) | âœ… Automatique (chargement) |
| **Output** | Verbeux | Verbeux | ğŸšï¸ 3 modes (minimal/intermediate/debug) |

---

## ğŸš€ Usage Rapide

### Nouvelle conversation

```python
result = chat_agent(
    message="Bonjour, aide-moi Ã  analyser des donnÃ©es SQL",
    model="gpt-5.2",
    tools=["sqlite_db", "date", "math"],
    output_mode="minimal"
)

# result:
# {
#   "success": true,
#   "response": "Bonjour ! Je suis prÃªt Ã  vous aider...",
#   "tools_used": []
# }

# Extraire le thread_id pour la suite
thread_id = result.get("thread_id")  # (prÃ©sent en mode intermediate/debug)
```

### Continuation (mÃªme conversation)

```python
result2 = chat_agent(
    message="CrÃ©e une table 'produits' avec 5 colonnes",
    model="gpt-5.2",
    tools=["sqlite_db"],
    thread_id=thread_id,  # â† Reprise du contexte
    output_mode="intermediate"
)

# result2:
# {
#   "success": true,
#   "response": "J'ai crÃ©Ã© la table 'produits' avec...",
#   "tools_used": ["sqlite_db"],
#   "thread_id": "thread_stream_...",
#   "operations_summary": [...],
#   "context_info": {"message_count": 4, "total_iterations": 2}
# }
```

---

## ğŸ“‹ ParamÃ¨tres

| ParamÃ¨tre | Type | Requis | DÃ©faut | Description |
|-----------|------|--------|--------|-------------|
| `message` | string | âœ… | - | Message utilisateur |
| `model` | string | âœ… | - | Nom du modÃ¨le (ex: `gpt-5.2`, `qwen3-next:80b`) |
| `tools` | array | âŒ | `[]` | Outils MCP disponibles (si vide = conversation pure) |
| `thread_id` | string | âŒ | - | ID du thread pour continuer une conversation |
| `output_mode` | enum | âŒ | `intermediate` | Niveau de dÃ©tail (`minimal`, `intermediate`, `debug`) |
| `max_iterations` | integer | âŒ | 10 | Limite d'itÃ©rations (anti-boucle infinie) |
| `timeout` | integer | âŒ | 300 | Timeout global en secondes |
| `temperature` | float | âŒ | 0.5 | TempÃ©rature LLM (0.0-2.0) |
| `system_prompt` | string | âŒ | (dÃ©faut) | Prompt systÃ¨me custom |
| `parallel_execution` | boolean | âŒ | `true` | ExÃ©cuter tools indÃ©pendants en parallÃ¨le |

---

## ğŸšï¸ Modes de Sortie

### Mode `minimal` (pour chatbots)

Retour le plus simple possible :

```json
{
  "success": true,
  "response": "Voici ma rÃ©ponse...",
  "tools_used": ["date", "math"]
}
```

**Usage** : UI chatbot oÃ¹ seule la rÃ©ponse finale compte.

---

### Mode `intermediate` (pour production)

Ajoute contexte utile pour debug/logs :

```json
{
  "success": true,
  "response": "Voici ma rÃ©ponse...",
  "tools_used": ["sqlite_db"],
  "thread_id": "thread_stream_ABC123",
  "operations_summary": [
    {"iteration": 1, "tools": ["sqlite_db"], "count": 1},
    {"iteration": 2, "tools": [], "count": 0}
  ],
  "context_info": {
    "message_count": 6,
    "total_iterations": 2
  }
}
```

**Usage** : Applications production avec besoin de traÃ§abilitÃ©.

---

### Mode `debug` (pour dÃ©veloppement)

DÃ©tails complets (arguments, rÃ©sultats, usage) :

```json
{
  "success": true,
  "response": "...",
  "tools_used": ["sqlite_db"],
  "thread_id": "thread_stream_ABC123",
  "iterations": 2,
  "operations": [
    {
      "iteration": 1,
      "tool_calls": [
        {
          "name": "sqlite_db",
          "arguments": "{\"operation\":\"create_db\",\"name\":\"test.db\"}",
          "result": {"success": true, "path": "sqlite3/test.db"}
        }
      ]
    }
  ],
  "usage": {
    "prompt_tokens": 1500,
    "completion_tokens": 200,
    "total_tokens": 1700
  },
  "context_info": {...},
  "transcript_snapshot": [...]  // 10 derniers messages
}
```

**Usage** : Debug, tests, analyse de performance.

---

## ğŸ›¡ï¸ Gestion du Contexte

### VÃ©rification prÃ©ventive

Avant chaque appel LLM, `chat_agent` **estime le nombre de tokens** de l'historique et compare Ã  la limite du modÃ¨le.

```python
# Si contexte > 90% de la limite du modÃ¨le
{
  "error": "ContextTooLarge",
  "message": "Conversation too long for model gpt-5.2 (estimated: 95000 tokens, limit: 100000)",
  "hint": "Please start a new thread",
  "estimated_tokens": 95000,
  "context_limit": 100000
}
```

**Action** : L'utilisateur doit crÃ©er un nouveau thread (impossible de truncate automatiquement car perte de cohÃ©rence).

---

## ğŸ”§ Architecture Technique

```
agent.py (entry point)
  â†“
  [Validation] (validators.py, model_validator.py)
  â†“
  [Thread Loading] (platform_api.py â†’ GET /user/threads/{id})
  â†“
  [Context Check] (thread_utils.py â†’ estimate_tokens)
  â†“
  loop.py (multi-turn)
    â†“
    1. Call LLM (platform_api.py â†’ streaming.py)
    2. If tool_calls:
       â†“
       executor.py (parallel/sequential)
       â†“
       Add tool results to transcript (thread_chain.py)
    3. Repeat until finish_reason == "stop"
  â†“
  [Output Formatting] (output_builder.py)
```

**Composants critiques** :

- `thread_chain.py` : Gestion des IDs (`id`, `parentId`, `level`) pour Ã©viter les branches
- `streaming.py` : Parsing SSE pour extraction `tool_calls`, `thread_id`, `usage`
- `thread_utils.py` : Conversion historique Platform â†’ messages OpenAI

---

## âš ï¸ Limitations & Bonnes Pratiques

### âœ… Ã€ FAIRE

1. **Stocker le `thread_id`** : Indispensable pour reprendre la conversation
2. **Mode `minimal` pour chatbots** : Ã‰vite overhead inutile
3. **Surveillance du contexte** : Surveiller `context_info.message_count`
4. **TempÃ©rature basse pour workflows** : 0.3-0.5 pour tÃ¢ches dÃ©terministes

### âš ï¸ Ã€ Ã‰VITER

1. **Perdre le `thread_id`** : Impossible de reprendre sans
2. **Conversations infinies** : Au-delÃ  de 50-100 messages, crÃ©er un nouveau thread
3. **Tools inutiles** : Ne fournir que les tools nÃ©cessaires (< 10 idÃ©alement)
4. **Mode `debug` en production** : CoÃ»teux en bande passante

---

## ğŸ§ª Tests

### Test 1 : Conversation simple (sans tools)

```bash
curl -X POST http://127.0.0.1:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "chat_agent",
    "params": {
      "message": "Bonjour, quelle heure est-il ?",
      "model": "gpt-4o-mini",
      "tools": [],
      "output_mode": "minimal"
    }
  }'
```

**Attendu** : RÃ©ponse directe (pas de tool use).

---

### Test 2 : Workflow avec tools

```bash
# Tour 1 : CrÃ©er une DB
curl -X POST http://127.0.0.1:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "chat_agent",
    "params": {
      "message": "CrÃ©e une DB SQLite nommÃ©e test.db",
      "model": "gpt-5.2",
      "tools": ["sqlite_db"],
      "output_mode": "intermediate"
    }
  }'

# Extraire thread_id de la rÃ©ponse

# Tour 2 : InsÃ©rer des donnÃ©es
curl -X POST http://127.0.0.1:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "chat_agent",
    "params": {
      "message": "InsÃ¨re 3 lignes dans une table users",
      "model": "gpt-5.2",
      "tools": ["sqlite_db"],
      "thread_id": "thread_stream_...",
      "output_mode": "intermediate"
    }
  }'
```

**Attendu** : Le LLM se souvient de `test.db` et l'utilise.

---

### Test 3 : Erreur contexte trop grand

```python
# Simuler un thread avec 1000+ messages (via boucle)
for i in range(200):
    result = chat_agent(
        message=f"Message {i}",
        model="gpt-4o-mini",
        thread_id=thread_id,
        tools=[]
    )

# AprÃ¨s ~150-200 messages (selon modÃ¨le)
# result["error"] == "ContextTooLarge"
```

---

## ğŸ“š Voir Aussi

- **SpÃ©cification complÃ¨te** : `docs/chat_agent/SPECIFICATION.md`
- **Spec JSON** : `src/tool_specs/chat_agent.json`
- **Ancienne version** : `src/tools/_call_llm_agent/` (pour comparaison)
- **Guide Threading API** : `docs/chat-completion-threading-guide.md`
