


<div align="center">

<!-- Local logo for reliability (placed in assets/) -->
<img src="assets/LOGO_DRAGONFLY_HD.jpg" alt="Dragonfly logo" width="120" style="background:#ffffff; padding:6px; border-radius:8px;" />

# ğŸ‰ Dragonfly MCP Server

Serveur MCP multiâ€‘outils, rapide et extensible, propulsÃ© par FastAPI. DÃ©couverte automatique des tools, exÃ©cution sÃ©curisÃ©e, orchestrateur LLM avancÃ©, et panneau de contrÃ´le web.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)
![Python](https://img.shields.io/badge/Python-3.11%2B-3776AB)
![FastAPI](https://img.shields.io/badge/FastAPI-%F0%9F%9A%80-009688)
![Status](https://img.shields.io/badge/Status-Active-success)

</div>

---

## âœ¨ Vue dâ€™ensemble
Dragonfly MCP Server expose des Â« tools Â» (au format OpenAI tools) via des endpoints HTTP simples:
- DÃ©couverte automatique des outils sous `src/tools/`
- ExÃ©cution dâ€™un tool via `POST /execute`
- Orchestration LLM en 2 phases via `call_llm` (avec usage cumulatif)
- Panneau de contrÃ´le web pour configurer et tester (`/control`)

> Pour les dÃ©tails dâ€™API (endpoints, sÃ©rialisation JSON, etc.), consultez aussi [src/README.md](./src/README.md).

---

## ğŸ“š Sommaire
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Demo rapide](#-demo-rapide)
- [Installation](#-installation)
- [DÃ©marrage](#-dÃ©marrage)
- [Prerequis Python](#-prerequis-python)
- [Endpoints](#-endpoints)
- [Outils inclus](#-outils-inclus)
- [Orchestrateur LLM (call_llm)](#-orchestrateur-llm-call_llm)
- [Configuration](#-configuration)
- [SÃ©curitÃ©](#-sÃ©curitÃ©)
- [Structure du projet](#-structure-du-projet)
- [Migration notes](#-migration-notes)
- [Pour les LLM Â« dÃ©veloppeurs Â»](#-pour-les-llm-dÃ©veloppeurs)
- [Feuille de route](#-feuille-de-route)
- [Licence](#-licence)

---

## ğŸš€ FonctionnalitÃ©s
- Autoâ€‘reload des tools (dÃ©tection de nouveaux fichiers dans `src/tools/`)
- JSON Â« sÃ»r Â»: grands entiers, NaN/Infinity sanitisÃ©s
- Orchestration LLM streaming en 2 phases (avec cumul dâ€™usage multiâ€‘niveaux)
- Panneau de contrÃ´le web (`/control`)
- Outils prÃªts Ã  lâ€™emploi: Git/GitHub, SQLite, PDF, Date/Heure, Math (HP), GitBook, Reddit, Universal Doc Scraper, Script Executor, FFmpeg frames, Academic Research, etc.

---

## âš¡ Demo rapide
ExÃ©cuter un tool en une requÃªte:

```bash
curl -s -X POST http://127.0.0.1:8000/execute \
 -H 'Content-Type: application/json' \
 -d '{"tool":"date","params":{"operation":"today"}}'
```

Orchestrer un LLM (rÃ©ponse texte)Â :

```bash
curl -s -X POST http://127.0.0.1:8000/execute \
 -H 'Content-Type: application/json' \
 -d '{
   "tool":"call_llm",
   "params":{ "message":"Dis bonjour en franÃ§ais.", "model":"gpt-4o" }
 }'
```

Lister les tools disponibles:

```bash
curl -s http://127.0.0.1:8000/tools
```

---

## ğŸ›  Installation
PrÃ©requis: Python 3.11 ou 3.12. VÃ©rifiez avec: `python3 --version`. Si votre version est < 3.11, installez via pyenv/conda ou depuis python.org.

```bash
git clone https://github.com/FranckDubray/dragonfly-mcp-server.git
cd dragonfly-mcp-server
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\Activate.ps1
pip install -U pip
pip install -e ".[dev]"
```

## â–¶ï¸ DÃ©marrage
- Linux/macOS: `./scripts/dev.sh`
- Windows: `scripts\dev.ps1`

Par dÃ©faut: http://127.0.0.1:8000

Panneau de contrÃ´le: http://127.0.0.1:8000/control

---

## ğŸ Prerequis Python
- Version minimale recommandÃ©e: Python 3.11 ou 3.12.
- Le projet utilise des fonctionnalitÃ©s modernes (annotations/typing, comportement de json/ints, etc.) pouvant Ã©chouer avec des versions trop anciennes.
- Les scripts de dÃ©marrage vÃ©rifient automatiquement la version installÃ©e et abortent si la version est trop ancienne.

> Astuce: utilisez pyenv pour installer la bonne version, ou conda/mamba.

---

## ğŸ”— Endpoints
- `GET /tools` â€” liste des tools (spec incluse). Ajouter `?reload=1` pour forcer un rescannage.
- `POST /execute` â€” exÃ©cuter un tool: `{ tool: string, params: object }`
- `GET /config` / `POST /config` â€” lire/Ã©crire la configuration (.env)
- `GET /control` â€” panneau HTML
- `GET /control.js` â€” script du panneau

DÃ©tails Ã©tendus: [src/README.md](./src/README.md)

---

## ğŸ§ª Outils inclus
- `call_llm`: orchestrateur LLM (2 phases, usage cumulatif)
- `math`: calcul numÃ©rique/HP, symbolique, algÃ¨bre linÃ©aire (+ extensions), solveurs, sÃ©ries, nombres premiers, sommes
- `date`: now/today, diff, add, format, parse, weekday, week_number
- `git`: GitHub API + Git local (opÃ©rations sÃ©curisÃ©es)
- `gitbook`: discovery/lecture/search GitBook
- `sqlite_db`: SQLite chroot (bases sous `<projet>/sqlite3`)
- `pdf_search` / `pdf2text`
- `reddit_intelligence`
- `universal_doc_scraper`
- `script_executor`: exÃ©cution de scripts Python sandboxÃ©s orchestrant des tools
- `ffmpeg_frames`: extraction dâ€™images/frames dâ€™une vidÃ©o via FFmpeg (dÃ©tection native PyAV, debug de similaritÃ©, exec_time_sec)
- `academic_research_super`: pipeline avancÃ© de recherche acadÃ©mique (agrÃ©gation, scraping, synthÃ¨se)

Specs JSON (OpenAI tools) correspondantes dans `src/tool_specs/`.

---

## ğŸ§  Orchestrateur LLM (`call_llm`)
- 1er stream (avec tools): collecte des `tool_calls`, exÃ©cution cÃ´tÃ© serveur
- 2e stream (sans tools): gÃ©nÃ©ration du texte final
- Usage cumulatif: additionne automatiquement les usages des 2 streams et de tous les appels imbriquÃ©s (ex: A â†’ B â†’ sonar)
- ParamÃ¨tres clÃ©s: `message`, `model`, `tool_names` (liste des tools exposÃ©s au modÃ¨le), `promptSystem`, `debug`

---

## âš™ï¸ Configuration
Variables principales:
- RÃ©seau/serveur: `MCP_HOST`, `MCP_PORT`, `LOG_LEVEL`
- ExÃ©cution: `EXECUTE_TIMEOUT_SEC`, `AUTO_RELOAD_TOOLS`, `RELOAD`
- LLM: `AI_PORTAL_TOKEN`, `LLM_ENDPOINT`, `LLM_REQUEST_TIMEOUT_SEC`, `LLM_RETURN_DEBUG`, `MCP_URL`
- JSON/entiers: `BIGINT_AS_STRING`, `BIGINT_STR_THRESHOLD`, `PY_INT_MAX_STR_DIGITS`
- Divers: `GITHUB_TOKEN`

Configurer via `/control` (recommandÃ©) ou via `.env`.

---

## ğŸ”’ SÃ©curitÃ©
- SQLite chroot: DBs sous `<projet>/sqlite3` (noms validÃ©s)
- Git local: opÃ©rations limitÃ©es Ã  la racine du projet
- `script_executor`: sandbox stricte (pas dâ€™accÃ¨s non autorisÃ©)
- Safe JSON: sÃ©rialisation robuste (NaN/Infinity, trÃ¨s grands entiers)

---

## ğŸ—‚ Structure du projet
```
src/
  app_factory.py     # FastAPI app, endpoints, auto-reload, Safe JSON
  server.py          # EntrÃ©e (Uvicorn) â€” crÃ©e lâ€™app et lance le serveur
  config.py          # .env (load/save), masquage des secrets
  tools/             # Tous les tools (run() + spec())
    _call_llm/       # Orchestrateur LLM: core, payloads, streaming, http_client...
    _math/           # ArithmÃ©tique, symbolique, proba, algÃ¨bre linÃ©aire, etc.
    _ffmpeg/         # Utilitaires FFmpeg (frames, conversion)
    _script/         # Sandbox du ScriptExecutor
  tool_specs/        # Specs JSON canoniques (OpenAI tools)
  README.md          # Doc API interne (endpoints + composants)
```

---

## ğŸ§­ Migration notes
- Python 3.11+ requis (scripts et metadata lâ€™imposent).
- Les scripts de dev chargent maintenant `.env` avant lâ€™install et le lancement.
- Le dossier top-level `script_executor/` est ignorÃ© par Git: dÃ©placez vos scripts utilisateurs dans un dossier hors repo si besoin.
- Pour les fonctionnalitÃ©s math avancÃ©es, installez `sympy`; pour haute prÃ©cision, `mpmath` (optionnel).

---

## ğŸ‘©â€ğŸ’» Pour les LLM Â« dÃ©veloppeurs Â»
Vous modifiez/Ã©tendez le dÃ©pÃ´t ? Lisez ce guide:
- [LLM_DEV_GUIDE.md](./LLM_DEV_GUIDE.md)
  - Conventions, invariants, checklists, piÃ¨ges Ã  Ã©viter
  - RÃ¨gles de spec JSON (parameters = object, arrays â†’ items)
  - DÃ©tails sur `call_llm` (streaming, usage cumulatif) et le JSON sÃ»r

---

## ğŸ—ºï¸ Feuille de route (extraits)
- [ ] Tests automatisÃ©s (tools + orchestrateur)
- [ ] Exemples interactifs dans `/control`
- [ ] IntÃ©gration dâ€™auth facultative sur les endpoints sensibles
- [ ] Export de mÃ©triques (Prometheus)

Contributions bienvenues â€” issues & PRs !

---

## ğŸ“„ Licence
MIT â€” voir [LICENSE](./LICENSE)
