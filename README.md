# üêâ Dragonfly MCP Server

Serveur MCP (Model Context Protocol) moderne avec 45+ tools pour LLM, interface vocale temps r√©el et orchestration workers asynchrones.

---

## üöÄ Quickstart

```bash
# Installation
pip install -e .

# Configuration
cp .env.example .env
# √âditer .env avec vos cl√©s API

# D√©marrer le serveur (au choix)
./scripts/dev.sh          # Recommand√© (Unix/Mac)
./scripts/dev.ps1         # Windows PowerShell
python src/server.py      # Direct

# Interfaces web (m√™me serveur, port 8000)
http://localhost:8000/control     # Control Panel (tools)
http://localhost:8000/workers     # Workers Vocaux (Realtime)
```

---

## ‚ú® Nouveaut√©s

### 1.27.5
- Nouveaux tools:
  - Lichess (Public API, read‚Äëonly): profils, perfs, √©quipes, parties, tournois, leaderboards, puzzles.
  - Stockfish (Auto‚Äë75): √©valuation de position et analyse de partie avec auto‚Äëconfiguration (~75% des ressources), MultiPV, budget‚Äëtemps global.
- Catalog/tools: specs charg√©es dans le registre (src/tool_specs/).

### 1.27.4
- üîî Sonnerie ‚ÄúSkype-like‚Äù (‚âà400/450 Hz), volume unifi√© (slider 50%)
- üß† VAD: coupure IA imm√©diate √† la parole utilisateur
- üìà Process overlay (Mermaid): pr√©chargement, KPIs derni√®re heure, replay ‚Äúmagn√©tophone‚Äù, alerte d‚Äôincoh√©rence d√©taill√©e
- üü©üüßüü• Cartes color√©es par activit√© (0‚Äì15 vert, 16‚Äì40 orange, >40 rouge)

---

## üß© Architecture
- FastAPI (routes workers) + SafeJSON + tool registry
- SQLite local (sqlite3/worker_*.db), scan automatique
- Config Realtime hybride (DB‚Äëfirst, fallback .env)

---

## üß™ D√©mo rapide
1. Ouvrir `http://localhost:8000/workers`.
2. Cliquer üìû ‚ÄúAppeler‚Äù ‚Üí sonnerie ‚Üí session.
3. Parler: l‚ÄôIA se coupe aussit√¥t; reprend apr√®s ~silence stable.
4. Cliquer üß≠ ‚ÄúProcessus‚Äù ‚Üí sch√©ma Mermaid, n≈ìud courant, arguments, historique, replay.
5. Le curseur Volume contr√¥le sonnerie + voix IA.

---

## üìù Changelog
Voir `CHANGELOG.md`.


## üß© Worker config (config.py) ‚Äî variables d‚Äôinit centralis√©es (NOUVEAU)

Pour configurer un worker sans toucher √† son process.py, placez un fichier d√©di√©:

- Emplacement: `workers/<worker_name>/config.py`
- Cl√© requise: `CONFIG` (dict de variables de contexte)
- Cl√© optionnelle: `CONFIG_DOC` (dict var ‚Üí description lisible)
- Alternatives accept√©es (si vous pr√©f√©rez): `WORKER_CONFIG` ou `config` pour les valeurs, `CONFIG_DESC`/`DOC`/`DESCRIPTIONS` pour la doc.

Chargement & priorit√©
- L‚Äôorchestrateur charge `process.py` puis fusionne `config.py`.
- Priorit√©: les valeurs de `config.py` surchargent celles de `PROCESS.metadata` si une cl√© existe dans les deux.
- Les paires (valeur, description) sont expos√©es √† l‚ÄôUI et via l‚ÄôAPI (voir ci‚Äëdessous).

Exemple `workers/ai_curation_v2/config.py`
```python
CONFIG = {
    # R√©seau / I/O
    "http_timeout_sec": 120,  # Timeout HTTP (sec) pour les calls MCP (ex: call_llm)

    # LLM
    "llm_model": "gpt-4o-mini",
    "sonar_model": "sonar",
    "llm_temperature": 0.3,

    # Logique m√©tier
    "quality_threshold": 7,
    "max_retries": 3,

    # Sources
    "primary_sites": [
        "https://openai.com/index",
        "https://www.anthropic.com/news",
        "https://blog.google/technology/ai",
        "https://deepmind.google/discover/blog",
        "https://ai.meta.com/blog",
        "https://aws.amazon.com/blogs",
        "https://azure.microsoft.com/blog",
        "https://developer.nvidia.com/blog",
        "https://stability.ai/news",
        "https://arxiv.org"
    ],

    # Stockage
    "db_file": "worker_ai_curation_v2.db",
}

CONFIG_DOC = {
    "http_timeout_sec": "Timeout HTTP (sec) pour les appels MCP (call_llm, etc.). D√©faut 30s.",
    "llm_model": "Mod√®le LLM principal utilis√© pour le scoring et le formatage du rapport.",
    "sonar_model": "Mod√®le Sonar utilis√© pour la validation/collecte.",
    "llm_temperature": "Temp√©rature de g√©n√©ration LLM.",
    "quality_threshold": "Seuil de score minimal pour consid√©rer la curation satisfaisante.",
    "max_retries": "Nombre maximal de tentatives de re‚Äëscoring si la validation √©choue.",
    "primary_sites": "Liste des sites officiels prioritaires pour l‚Äôenrichissement/validation.",
    "db_file": "Nom du fichier SQLite pour persister l‚Äô√©tat et les rapports.",
}
```

R√©cup√©rer la config en live (API)
```json
{
  "tool": "py_orchestrator",
  "params": { "operation": "config", "worker_name": "ai_curation_v2" }
}
```
R√©ponse (extrait):
```json
{
  "accepted": true,
  "status": "ok",
  "worker_name": "ai_curation_v2",
  "metadata": { "http_timeout_sec": 120, "llm_model": "gpt-4o-mini", ... },
  "docs": { "http_timeout_sec": "Timeout HTTP...", ... },
  "truncated": false
}
```

Notes
- `http_timeout_sec` est lu par le runner pour configurer le timeout HTTP des tools MCP (fallback 30s). Pas besoin de variable d‚Äôenv globale.
- Vous pouvez d√©placer progressivement les variables de `PROCESS.metadata` vers `config.py`. Si une m√™me cl√© existe dans les deux, c‚Äôest `config.py` qui gagne.
- L‚ÄôAPI `status` continue de renvoyer `py.metadata` (pour compat) et `config` offre une vue d√©di√©e variables+docs.


---

## üß† Workers LLM ‚Äî meilleures pratiques (config/ dir + prompts fichiers + API config)

Objectif
- Rendre les workers LLM faciles √† maintenir et √† mettre √† jour en production: prompts en fichiers, config JSON, hot‚Äëreload, √©dition fine via API.

Architecture de config (directory-only)
- Tout vit dans le r√©pertoire du worker: `workers/<name>/config/` (plus de config.py √† la racine).
- Fichiers support√©s (charg√©s au runtime et en hot‚Äëreload):
  - `config/config.json` ‚Üí deep‚Äëmerge vers `process.metadata`.
  - `config/prompts/*.md|*.txt` ‚Üí inject√©s dans `process.metadata.prompts[<stem>]`.
  - `config/CONFIG_DOC.json` (optionnel) ‚Üí descriptions lisibles par l‚ÄôUI.

Exemple d‚Äôarborescence
```
workers/ai_curation_v2/
  process.py
  subgraphs/
  config/
    config.json
    CONFIG_DOC.json
    prompts/
      collect_sonar.md
      score_gpt.md
      validate_json.md
      output_fr.md
```

R√®gles de code (rappel Orchestrator)
- 1 appel par step: exactement un `env.tool(...)` OU un `env.transform(...)`.
- 0 appel dans un cond.
- Pas de `import/for/while/with/try/open/eval/exec` dans les steps/conds.
- Chaque fonction retourne `Next(...)` ou `Exit(...)`.

Lire les prompts dans les steps
```python
prompts = worker.get("prompts", {})
msg = str(prompts.get("collect_sonar") or "").format(FROM_ISO=str(cycle["dates"]["from"]))
out = env.tool("call_llm", model=worker.get("sonar_model"), messages=[{"role":"user","content":msg}], temperature=0.3)
```

Mettre √† jour la config avec l‚ÄôAPI (√©dition g√©n√©rique n niveaux)
- Tool: `py_orchestrator`, op: `config`.
- Cl√©s imbriqu√©es via `key_path` avec dot + indices + cl√©s quot√©es:
  - `a.b[2].c[0]["x.y"]`
- Stockage:
  - `storage:"file"`: persiste sous `workers/<name>/config/`.
    - `prompts.<name>` ‚Üí `config/prompts/<name>.md` (auto‚Äërouting).
    - autres cl√©s ‚Üí `config/config.json` (deep set cibl√©).
  - `storage:"inline"`: met √† jour le KV (live), sans √©crire de fichier.
- √âcriture fichier directe: `set.file` ‚Üí chemin relatif sous `config/`.

Exemples concrets
- Remplacer un prompt pr√©cis (persist√© en fichier `.md`):
```json
{
  "tool": "py_orchestrator",
  "params": {
    "operation": "config",
    "worker_name": "ai_curation_v2",
    "set": { "key_path": "prompts.collect_sonar", "value": "‚Ä¶nouveau prompt‚Ä¶", "storage": "file" }
  }
}
```
- Modifier un cap d‚ÄôURL sans toucher tout l‚Äôobjet (cl√© avec point):
```json
{
  "tool": "py_orchestrator",
  "params": {
    "operation": "config",
    "worker_name": "ai_curation_v2",
    "set": { "key_path": "primary_site_caps[\"https://x.ai/blog\"]", "value": 7000, "storage": "file" }
  }
}
```
- Ins√©rer/modifier profond√©ment (mix dict + array + cl√© quot√©e):
```json
{
  "tool": "py_orchestrator",
  "params": {
    "operation": "config",
    "worker_name": "ai_curation_v2",
    "set": { "key_path": "features[3].overrides[\"k.v\"].limits[0]", "value": {"max": 42, "enabled": true}, "storage": "file" }
  }
}
```
- √âcrire un fichier arbitraire sous `config/` (ex: drapeaux):
```json
{
  "tool": "py_orchestrator",
  "params": {
    "operation": "config",
    "worker_name": "ai_curation_v2",
    "set": { "file": "vars/feature_flags.json", "value": {"beta": true, "rollout": 0.25}, "create": true }
  }
}
```

Bonnes pratiques LLM
- Externaliser tous les prompts en `.md` (lisibles, versionnables). Injecter des variables `{FROM_ISO}`, `{NOW_ISO}`, `{ITEMS}`‚Ä¶ via `.format()` c√¥t√© step.
- Limiter les caps de scraping par site (5‚Äì8 KB) pour ne r√©cup√©rer que les derniers items.
- Toujours normaliser l‚Äôoutput LLM (`normalize_llm_output`) avant usage.
- Utiliser `validate` (Sonar) pour scorer la qualit√© et piloter les retries (threshold/max_retries).
- Exploiter `status`/debug pour voir `py.last_call` et `py.last_result_preview` en cas d‚Äôerreur.

Hot‚Äëreload & observabilit√©
- Toute modif dans `config/` est prise en compte sans restart (hot_reload=true).
- √Ä l‚Äôerreur step: logs enrichis (call + last_result_preview) en DB et KV.

Checklist
- [ ] Prompts dans `config/prompts/*.md`, pas en dur dans le code.
- [ ] Variables et sources dans `config/config.json` (pas de config.py racine).
- [ ] Steps respectent la r√®gle 1‚Äëcall/step et 0‚Äëcall/cond.
- [ ] `validate` OK; `graph.render.mermaid=true` lisible; debug.step fonctionne.
- [ ] `operation=config` test√© pour modifier un prompt et une cl√© imbriqu√©e.
