# üîÑ Reprise de session - Ollama Vision Debug

## üìã Contexte

Nous travaillons sur le projet **dragonfly-mcp-server** (serveur MCP avec 37 tools).

**Probl√®me actuel** : Le tool `ollama_local` ne g√®re pas correctement les images pour les mod√®les de vision (llava:13b).

---

## üêõ Diagnostic effectu√©

### Tests r√©alis√©s

**Script de test** : `test_gemini_image.sh`  
**Image test** : `docs/gemini_image_68dfaca1489e1.png`

**R√©sultats** :
- ‚ùå **TEST 1** (via tool `operation=chat`) : llava dit "je ne vois pas l'image"
- ‚è≥ **TEST 2** (via tool `operation=generate`) : en attente r√©sultats
- ‚è≥ **TEST 3** (Ollama direct `/api/generate`) : en attente r√©sultats

### D√©couverte importante

Via tests curl directs :
- ‚úÖ **`/api/generate`** : fonctionne avec images (llava d√©crit correctement)
- ‚ùå **`/api/chat`** : NE supporte PAS les images (llava dit qu'il ne voit pas)

**Source** : Documentation Ollama officielle confirme que seul `/api/generate` supporte les images.

---

## üîß Fix appliqu√©

### Modifications dans `src/tools/_ollama_local/core.py`

**Fonction `handle_chat()`** modifi√©e pour auto-basculer vers `generate` quand images pr√©sentes :

```python
def handle_chat(..., image_files=...):
    # Process images
    images_b64 = _process_images(image_urls, image_files)
    
    # CRITICAL: /api/chat doesn't support images
    # Switch to /api/generate when images present
    if images_b64:
        # Extract last user message as prompt
        prompt = messages[-1]["content"]
        
        # Use generate endpoint instead
        return local_client.generate(
            model=model,
            prompt=prompt,
            images=images_b64,
            ...
        )
    
    # No images: use chat normally
    return local_client.chat(...)
```

**Mais le TEST 1 montre que √ßa ne marche pas !** La r√©ponse contient `"message": {...}` (format chat), pas `"response": "..."` (format generate).

---

## ‚ùì Probl√®me actuel

**Le switch vers `generate` ne s'active pas** ou **les images ne sont pas pass√©es correctement**.

**Hypoth√®ses** :
1. `images_b64` est vide (non d√©tect√© comme truthy)
2. Le code ne passe pas par le `if images_b64:` 
3. Les images sont skip√©es quelque part dans `_process_images()`
4. Le tool appelle le mauvais handler

---

## üéØ Actions n√©cessaires

### Imm√©diat
1. Attendre r√©sultats complets des TEST 2 et TEST 3
2. Comparer les 3 sorties pour localiser le bug
3. Ajouter du debug logging dans `handle_chat()` pour voir si le switch est d√©clench√©

### Debug √† ajouter
```python
if images_b64:
    print(f"DEBUG: Switching to generate. Images count: {len(images_b64)}")
    print(f"DEBUG: Prompt extracted: {prompt[:100]}...")
```

---

## üìÇ Fichiers modifi√©s (non committ√©s)

- `src/tools/call_llm.py` - Fix chemin images (PROJECT_ROOT)
- `src/tools/_ollama_local/core.py` - Auto-switch vers generate + gestion images
- `src/tools/_ollama_local/services/local_client.py` - R√©duction logs streaming
- `src/tool_specs/ollama_local.json` - Ajout param√®tres image_files/image_urls
- `CHANGELOG.md` - v1.17.2 document√©

**Script de test actif** : `test_gemini_image.sh`

---

## üìù Pour reprendre

**Prompt recommand√©** :

```
Je reprends le debug du tool ollama_local pour les images de vision (llava).

Fichiers workspace actuels : [liste automatique du workspace]

Situation :
- Test 1 (chat) √©choue : llava dit "je ne vois pas l'image" 
- Tests 2 et 3 en attente de r√©sultats
- Le switch auto vers generate ne semble pas fonctionner

Voici les r√©sultats complets du script test_gemini_image.sh :
[coller les 3 r√©sultats]

Analyse et propose un fix.
```

---

## üîë Variables d'environnement

```bash
# Partag√©es entre call_llm et ollama_local
LLM_MAX_IMAGE_FILE_BYTES=5000000  # Max 5 MB par image
LLM_MAX_IMAGE_COUNT=4              # Max 4 images
DOCS_ABS_ROOT=<auto-d√©tect√©>      # Racine projet/docs
```

---

## ‚öôÔ∏è Versions

- **Ollama** : local (localhost:11434)
- **Mod√®le test** : llava:13b
- **Python** : 3.11+
- **Serveur MCP** : FastAPI (port 8000)

---

**√âtat** : En cours de debug, fix partiellement appliqu√© mais non fonctionnel. Attente r√©sultats tests 2-3.
